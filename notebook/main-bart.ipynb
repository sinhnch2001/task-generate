{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install transformers datasets evaluate\n!pip -q install faiss_gpu\n!pip -q install nlp\n\nimport datasets\nimport functools\nimport math\nimport faiss  \nimport nlp  \nimport os  \nimport torch\nimport numpy as np\nimport pandas as pd\nimport torch.utils.checkpoint as checkpoint\n\nfrom tqdm import tqdm\nfrom time import time\nfrom random import choice, randint\nfrom torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\nfrom transformers import AdamW, AutoModel, AutoModelForSeq2SeqLM, AutoTokenizer, get_linear_schedule_with_warmup\n\npd.set_option(\"display.max_colwidth\", None)\n\n###############\n# ELI5 seq2seq model training\n###############\nclass ELI5DatasetS2S(Dataset):\n    def __init__(self, examples_array, make_doc_fun=None, extra_answer_threshold=3, document_cache=None, training=True):\n        self.training = training\n        self.data     = examples_array\n        self.make_doc_function = make_doc_fun\n        self.document_cache    = {} if document_cache is None else document_cache\n        assert not (make_doc_fun is None and document_cache is None)\n        \n        self.qa_id_list = [(i, 0) for i in range(len(self.data))]\n            \n    def __len__(self):\n        return len(self.qa_id_list)\n\n    def make_example(self, idx):\n        i, j = self.qa_id_list[idx]\n        example = self.data[i]\n        question = example[\"question\"] \n        answer = example[\"answers\"][j]\n        q_id = example[\"question_id\"]\n        if self.make_doc_function is not None:\n            self.document_cache[q_id] = self.document_cache.get(q_id, self.make_doc_function(example[\"question\"]))\n        document = self.document_cache[q_id]\n        in_st  = \"question: {} context: {}\".format(question.lower().replace(\" --t--\", \"\").strip(), document.lower().strip())\n        out_st = answer\n        return (in_st, out_st)\n\n    def __getitem__(self, idx):\n        return self.make_example(idx)\n\ndef make_qa_s2s_model(model_name=\"facebook/bart-base\", from_file=None, device=\"cuda:0\"):\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n    if from_file is not None:\n        param_dict = torch.load(from_file)  # has model weights, optimizer, and scheduler states\n        model.load_state_dict(param_dict[\"model\"])\n    return tokenizer, model\n\ndef make_qa_s2s_batch(qa_list, tokenizer, max_len=64, max_a_len=360, device=\"cuda:0\"):\n    q_ls = [q for q, a in qa_list]\n    a_ls = [a for q, a in qa_list]\n    q_toks = tokenizer.batch_encode_plus(q_ls, max_length=max_len, pad_to_max_length=True)\n    q_ids, q_mask = (torch.LongTensor(q_toks[\"input_ids\"]).to(device),\n                     torch.LongTensor(q_toks[\"attention_mask\"]).to(device))\n    a_toks = tokenizer.batch_encode_plus(a_ls, max_length=min(max_len, max_a_len), pad_to_max_length=True)\n    a_ids, a_mask = (torch.LongTensor(a_toks[\"input_ids\"]).to(device),\n                     torch.LongTensor(a_toks[\"attention_mask\"]).to(device))\n    labels = a_ids[:, 1:].contiguous().clone()\n    labels[a_mask[:, 1:].contiguous() == 0] = -100\n    model_inputs = {\"input_ids\": q_ids,\n                    \"attention_mask\": q_mask,\n                    \"decoder_input_ids\": a_ids[:, :-1].contiguous(),\n                    \"labels\": labels}\n    return model_inputs\n\ndef train_qa_s2s_epoch(model, dataset, tokenizer, optimizer, scheduler, args, e=0, curriculum=False):\n    model.train()\n    \n    # make iterator\n    if curriculum:\n        train_sampler = SequentialSampler(dataset)\n    else:\n        train_sampler = RandomSampler(dataset)\n        \n    model_collate_fn = functools.partial(make_qa_s2s_batch, tokenizer=tokenizer, max_len=args.max_length, device=\"cuda:0\")\n    data_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=train_sampler, collate_fn=model_collate_fn)\n    epoch_iterator = tqdm(data_loader, desc=\"Iteration\", disable=True)\n    \n    # accumulate loss since last print\n    loc_steps = 0\n    loc_loss = 0.0\n    st_time = time()\n    \n    for step, batch_inputs in enumerate(epoch_iterator):\n        pre_loss = model(**batch_inputs)[0]\n        loss = pre_loss.sum() / pre_loss.shape[0]\n        loss.backward()\n        \n        # optimizer\n        if step % args.backward_freq == 0:\n            optimizer.step()\n            scheduler.step()\n            model.zero_grad()\n            \n        # some printing within the epoch\n        loc_loss += loss.item()\n        loc_steps += 1\n        if step % args.print_freq == 0 or step == 1:\n            print(\"{:2d} {:5d} of {:5d} \\t L: {:.3f} \\t -- {:.3f}\".format(\n                    e, step, len(dataset) // args.batch_size, loc_loss / loc_steps, time() - st_time,))\n            loc_loss  = 0\n            loc_steps = 0\n\ndef eval_qa_s2s_epoch(model, dataset, tokenizer, args):\n    model.eval()\n    \n    # make iterator\n    train_sampler = SequentialSampler(dataset)\n    model_collate_fn = functools.partial(make_qa_s2s_batch, tokenizer=tokenizer, \n                                         max_len=args.max_length, device=\"cuda:0\")\n    data_loader = DataLoader(dataset, batch_size=args.batch_size, sampler=train_sampler, collate_fn=model_collate_fn)\n    epoch_iterator = tqdm(data_loader, desc=\"Iteration\", disable=True)\n    \n    # accumulate loss since last print\n    loc_steps = 0\n    loc_loss  = 0.0\n    st_time   = time()\n    with torch.no_grad():\n        for step, batch_inputs in enumerate(epoch_iterator):\n            pre_loss = model(**batch_inputs)[0]\n            loss = pre_loss.sum() / pre_loss.shape[0]\n            loc_loss += loss.item()\n            loc_steps += 1\n            if step % args.print_freq == 0:\n                print(\"{:5d} of {:5d} \\t L: {:.3f} \\t -- {:.3f}\".format(\n                        step, len(dataset) // args.batch_size, loc_loss / loc_steps, time() - st_time))\n    print(\"Total \\t L: {:.3f} \\t -- {:.3f}\".format(loc_loss / loc_steps, time() - st_time))\n\n\ndef train_qa_s2s(qa_s2s_model, qa_s2s_tokenizer, s2s_train_dset, s2s_valid_dset, s2s_args):\n    s2s_optimizer = AdamW(qa_s2s_model.parameters(), lr=s2s_args.learning_rate, eps=1e-8)\n    s2s_scheduler = get_linear_schedule_with_warmup(s2s_optimizer, num_warmup_steps=400,\n                                                    num_training_steps=(s2s_args.num_epochs + 1) * math.ceil(len(s2s_train_dset) / s2s_args.batch_size),)\n    for e in range(s2s_args.num_epochs):\n        train_qa_s2s_epoch(qa_s2s_model,s2s_train_dset,qa_s2s_tokenizer,s2s_optimizer,\n                           s2s_scheduler,s2s_args,e,curriculum=(e == 0),)\n        m_save_dict = {\"model\"    : qa_s2s_model.module.state_dict()\n                                    if hasattr(qa_s2s_model, 'module') else qa_s2s_model.state_dict(),\n                       \"optimizer\": s2s_optimizer.state_dict(),\n                       \"scheduler\": s2s_scheduler.state_dict()}\n        \n        print(\"Saving model {}\".format(s2s_args.model_save_name))\n        eval_qa_s2s_epoch(qa_s2s_model, s2s_valid_dset, qa_s2s_tokenizer, s2s_args)\n        torch.save(m_save_dict, \"{}.pth\".format(s2s_args.model_save_name))\n\n\n# generate answer from input \"question: ... context: <p> ...\"\ndef qa_s2s_generate(question_doc, qa_s2s_model, qa_s2s_tokenizer, num_answers=1, num_beams=None,\n                    min_len=64, max_len=256, do_sample=False,temp=1.0, top_p=None, top_k=None,\n                    max_input_length=512, device=\"cuda:0\"):\n    \n    model_inputs = make_qa_s2s_batch([(question_doc, \"A\")], qa_s2s_tokenizer, \n                                       max_input_length, device=device)\n    \n    n_beams = num_answers if num_beams is None else max(num_beams, num_answers)\n    model = qa_s2s_model.module if hasattr(qa_s2s_model, 'module') else qa_s2s_model \n    generated_ids = model.generate( input_ids=model_inputs[\"input_ids\"],\n                                           attention_mask=model_inputs[\"attention_mask\"],\n                                           min_length=min_len,max_length=max_len,\n                                           do_sample=do_sample, early_stopping=True,\n                                           num_beams=1 if do_sample else n_beams,\n                                           temperature=temp,top_k=top_k,top_p=top_p,\n                                           eos_token_id=qa_s2s_tokenizer.eos_token_id,\n                                           no_repeat_ngram_size=3,\n                                           num_return_sequences=num_answers,\n                                           decoder_start_token_id=qa_s2s_tokenizer.bos_token_id)\n    return [qa_s2s_tokenizer.decode(ans_ids, skip_special_tokens=True).strip() for ans_ids in generated_ids]\n\n\n###############\n# ELI5-trained retrieval model usage\n###############\ndef embed_passages_for_retrieval(passages, tokenizer, qa_embedder, max_length=128, device=\"cuda:0\"):\n    a_toks = tokenizer.batch_encode_plus(passages, max_length=max_length, pad_to_max_length=True)\n    a_ids, a_mask = (torch.LongTensor(a_toks[\"input_ids\"]).to(device),\n                     torch.LongTensor(a_toks[\"attention_mask\"]).to(device))\n    \n    with torch.no_grad():\n        a_reps = qa_embedder.embed_answers(a_ids, a_mask).cpu().type(torch.float)\n    return a_reps.numpy()\n\n\ndef embed_questions_for_retrieval(q_ls, tokenizer, qa_embedder, device=\"cuda:0\"):\n    q_toks = tokenizer.batch_encode_plus(q_ls, max_length=128, pad_to_max_length=True)\n    q_ids, q_mask = (torch.LongTensor(q_toks[\"input_ids\"]).to(device),\n                     torch.LongTensor(q_toks[\"attention_mask\"]).to(device))\n    \n    with torch.no_grad():\n        q_reps = qa_embedder.embed_questions(q_ids, q_mask).cpu().type(torch.float)\n    return q_reps.numpy()\n\n\ndef make_qa_dense_index(qa_embedder,tokenizer,passages_dset,batch_size=512,max_length=128,\n                        index_name=\"kilt_passages_reps.dat\",dtype=\"float32\",device=\"cuda:0\"):\n    st_time = time()\n    fp = np.memmap(index_name, dtype=dtype, mode=\"w+\", shape=(passages_dset.num_rows, 128))\n    n_batches = math.ceil(passages_dset.num_rows / batch_size)\n    print(\"Data size  = \", passages_dset.num_rows)\n    print(\"Batch_size = \", batch_size)\n    print(\"n_batch    = \", n_batches)\n    for i in range(n_batches):\n        passages = [p for p in passages_dset[i * batch_size : (i + 1) * batch_size][\"passage_text\"]]\n        reps = embed_passages_for_retrieval(passages, tokenizer, qa_embedder, max_length, device)\n        fp[i * batch_size : (i + 1) * batch_size] = reps\n        if i % 50 == 0:\n            print(i, time() - st_time)\n\n\ndef evaluate_retriever(qa_list, retriever_func, scoring_func, n_ret=10, verbose=False):\n    total_retriever_time = 0.0\n    total_retriever_score = 0.0\n    st_time = time()\n    for i, (question, answer) in enumerate(qa_list):\n        r_time = time()\n        retrieved_passages = retriever_func(question, n_ret)\n        total_retriever_time += time() - r_time\n        total_retriever_score += scoring_func(retrieved_passages, answer)\n        if verbose and ((i + 1) % 500 == 0 or i <= 1):\n            print(\"{:03d}: S-{:.4f} T-{:.4f} | {:.2f}\".format(\n                    i + 1, total_retriever_score / (i + 1), total_retriever_time / (i + 1), time() - st_time))\n    return {\"idf_recall\": total_retriever_score / (i + 1), \"retrieval_time\": total_retriever_time / (i + 1)}\n\n\n# build a support document for the question out of Wikipedia snippets\ndef query_qa_dense_index(question, qa_embedder, tokenizer, wiki_passages, \n                         wiki_index, n_results=10, min_length=20, device=\"cuda:0\"):\n    q_rep = embed_questions_for_retrieval([question], tokenizer, qa_embedder, device=device)\n    D, I = wiki_index.search(q_rep, 2 * n_results)\n    res_passages = [wiki_passages[int(i)] for i in I[0]]\n    support_doc = \"<P> \" + \" <P> \".join([p[\"passage_text\"] for p in res_passages])\n    res_list = [dict([(k, p[k]) for k in wiki_passages.column_names]) for p in res_passages]\n    res_list = [res for res in res_list if len(res[\"passage_text\"].split()) > min_length][:n_results]\n    for r, sc in zip(res_list, D[0]):\n        r[\"score\"] = float(sc)\n    return support_doc, res_list","metadata":{"_uuid":"3ef3ddd8-bca3-42c6-b0d4-31c1576f81cb","_cell_guid":"71602612-4946-4cdb-b55d-ae3a51b7b71f","collapsed":false,"jupyter":{"outputs_hidden":false},"scrolled":true,"execution":{"iopub.status.busy":"2023-01-31T22:29:33.427998Z","iopub.execute_input":"2023-01-31T22:29:33.428334Z","iopub.status.idle":"2023-01-31T22:30:26.561483Z","shell.execute_reply.started":"2023-01-31T22:29:33.428241Z","shell.execute_reply":"2023-01-31T22:30:26.560511Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# LOAD DATA ELI5 \nimport json\neli5_train = json.load(open('/kaggle/input/eli5-10-doc/ELI5_train_10_doc.json'))\neli5_valid = json.load(open('/kaggle/input/eli5-10-doc/ELI5_val_10_doc.json'))","metadata":{"_uuid":"3d35b5e0-8b0f-45b0-99c2-e1ea321ffe45","_cell_guid":"e078578c-016f-4c73-9aa0-d3ce466249ae","collapsed":false,"jupyter":{"outputs_hidden":false},"scrolled":true,"execution":{"iopub.status.busy":"2023-01-31T22:30:26.563654Z","iopub.execute_input":"2023-01-31T22:30:26.564179Z","iopub.status.idle":"2023-01-31T22:30:59.843084Z","shell.execute_reply.started":"2023-01-31T22:30:26.564149Z","shell.execute_reply":"2023-01-31T22:30:59.842034Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# PRE PROCESSING DOCS \neli5_train_docs = []\neli5_valid_docs = []\n\nfor example in eli5_train:\n    support_doc = \"<P> \" + \" <P> \".join([p for p in example[\"ctxs\"]])\n    eli5_train_docs += [(example['question_id'], support_doc)]\n\nfor example in eli5_valid:\n    support_doc = \"<P> \" + \" <P> \".join([p for p in example[\"ctxs\"]])\n    eli5_valid_docs += [(example['question_id'], support_doc)]\n\n# LOAD DOCS JSON for train and valid\ns2s_train_dset = ELI5DatasetS2S(eli5_train, document_cache=dict([(k, d) for k, d in eli5_train_docs]))\ns2s_valid_dset = ELI5DatasetS2S(eli5_valid, document_cache=dict([(k, d) for k, d in eli5_valid_docs]), training=False)","metadata":{"_uuid":"a15cdd25-af63-4d54-b3df-1da415e7cd33","_cell_guid":"b2418aa3-df97-4549-8ef7-1b5054905297","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-01-31T22:30:59.844468Z","iopub.execute_input":"2023-01-31T22:30:59.845332Z","iopub.status.idle":"2023-01-31T22:31:03.456051Z","shell.execute_reply.started":"2023-01-31T22:30:59.845289Z","shell.execute_reply":"2023-01-31T22:31:03.454852Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# CREATE ArgumentsS2S\nclass ArgumentsS2S():\n    def __init__(self):\n        self.batch_size = 8\n        self.backward_freq = 16\n        self.max_length = 1024\n        self.print_freq = 100\n        self.model_save_name = \"bart_eli5_task_1\"\n        self.learning_rate = 2e-4\n        self.num_epochs = 1\n\ns2s_args = ArgumentsS2S()\n\n# LOAD TOKENIZER and MODEL S2S\nqa_s2s_tokenizer, pre_model = make_qa_s2s_model(model_name=\"facebook/bart-base\",\n                                                from_file=None,\n                                                device=\"cuda:0\")\nqa_s2s_model = torch.nn.DataParallel(pre_model)\n\n# TRAINING\ntrain_qa_s2s(qa_s2s_model, qa_s2s_tokenizer, s2s_train_dset, s2s_valid_dset, s2s_args)","metadata":{"_uuid":"ca740e69-5867-4674-bbb6-cfc81b29c559","_cell_guid":"99407a99-c158-4485-a454-adee72d1d72c","collapsed":false,"jupyter":{"outputs_hidden":false},"scrolled":true,"execution":{"iopub.status.busy":"2023-01-31T22:31:57.478777Z","iopub.execute_input":"2023-01-31T22:31:57.479145Z","iopub.status.idle":"2023-02-01T07:37:40.661924Z","shell.execute_reply.started":"2023-01-31T22:31:57.479115Z","shell.execute_reply":"2023-02-01T07:37:40.660614Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":" 0     0 of 34079 \t L: 5.563 \t -- 7.685\n 0     1 of 34079 \t L: 6.276 \t -- 8.598\n 0   100 of 34079 \t L: 5.929 \t -- 99.711\n 0   200 of 34079 \t L: 5.358 \t -- 195.693\n 0   300 of 34079 \t L: 4.915 \t -- 292.306\n 0   400 of 34079 \t L: 4.658 \t -- 388.167\n 0   500 of 34079 \t L: 4.530 \t -- 483.915\n 0   600 of 34079 \t L: 4.423 \t -- 579.593\n 0   700 of 34079 \t L: 4.383 \t -- 675.216\n 0   800 of 34079 \t L: 4.290 \t -- 771.457\n 0   900 of 34079 \t L: 4.267 \t -- 867.311\n 0  1000 of 34079 \t L: 4.196 \t -- 963.102\n 0  1100 of 34079 \t L: 4.152 \t -- 1058.746\n 0  1200 of 34079 \t L: 4.075 \t -- 1154.485\n 0  1300 of 34079 \t L: 4.060 \t -- 1250.645\n 0  1400 of 34079 \t L: 4.009 \t -- 1346.211\n 0  1500 of 34079 \t L: 3.969 \t -- 1441.814\n 0  1600 of 34079 \t L: 3.961 \t -- 1537.603\n 0  1700 of 34079 \t L: 3.939 \t -- 1633.249\n 0  1800 of 34079 \t L: 3.931 \t -- 1729.476\n 0  1900 of 34079 \t L: 3.894 \t -- 1825.124\n 0  2000 of 34079 \t L: 3.882 \t -- 1920.803\n 0  2100 of 34079 \t L: 3.857 \t -- 2016.390\n 0  2200 of 34079 \t L: 3.854 \t -- 2112.150\n 0  2300 of 34079 \t L: 3.843 \t -- 2208.158\n 0  2400 of 34079 \t L: 3.830 \t -- 2303.902\n 0  2500 of 34079 \t L: 3.821 \t -- 2399.625\n 0  2600 of 34079 \t L: 3.830 \t -- 2495.410\n 0  2700 of 34079 \t L: 3.852 \t -- 2591.055\n 0  2800 of 34079 \t L: 3.805 \t -- 2686.942\n 0  2900 of 34079 \t L: 3.829 \t -- 2783.013\n 0  3000 of 34079 \t L: 3.778 \t -- 2878.688\n 0  3100 of 34079 \t L: 3.767 \t -- 2974.303\n 0  3200 of 34079 \t L: 3.804 \t -- 3070.215\n 0  3300 of 34079 \t L: 3.761 \t -- 3165.935\n 0  3400 of 34079 \t L: 3.797 \t -- 3262.113\n 0  3500 of 34079 \t L: 3.750 \t -- 3357.840\n 0  3600 of 34079 \t L: 3.730 \t -- 3453.554\n 0  3700 of 34079 \t L: 3.748 \t -- 3549.213\n 0  3800 of 34079 \t L: 3.764 \t -- 3644.915\n 0  3900 of 34079 \t L: 3.776 \t -- 3740.991\n 0  4000 of 34079 \t L: 3.737 \t -- 3836.723\n 0  4100 of 34079 \t L: 3.761 \t -- 3932.371\n 0  4200 of 34079 \t L: 3.723 \t -- 4027.986\n 0  4300 of 34079 \t L: 3.722 \t -- 4123.587\n 0  4400 of 34079 \t L: 3.724 \t -- 4219.835\n 0  4500 of 34079 \t L: 3.710 \t -- 4315.399\n 0  4600 of 34079 \t L: 3.705 \t -- 4411.273\n 0  4700 of 34079 \t L: 3.691 \t -- 4507.080\n 0  4800 of 34079 \t L: 3.698 \t -- 4602.825\n 0  4900 of 34079 \t L: 3.708 \t -- 4699.084\n 0  5000 of 34079 \t L: 3.707 \t -- 4794.872\n 0  5100 of 34079 \t L: 3.680 \t -- 4890.690\n 0  5200 of 34079 \t L: 3.682 \t -- 4986.362\n 0  5300 of 34079 \t L: 3.709 \t -- 5082.063\n 0  5400 of 34079 \t L: 3.716 \t -- 5178.384\n 0  5500 of 34079 \t L: 3.678 \t -- 5274.076\n 0  5600 of 34079 \t L: 3.683 \t -- 5369.922\n 0  5700 of 34079 \t L: 3.699 \t -- 5465.725\n 0  5800 of 34079 \t L: 3.719 \t -- 5561.313\n 0  5900 of 34079 \t L: 3.667 \t -- 5657.373\n 0  6000 of 34079 \t L: 3.685 \t -- 5753.038\n 0  6100 of 34079 \t L: 3.687 \t -- 5848.613\n 0  6200 of 34079 \t L: 3.641 \t -- 5944.349\n 0  6300 of 34079 \t L: 3.684 \t -- 6040.046\n 0  6400 of 34079 \t L: 3.678 \t -- 6135.630\n 0  6500 of 34079 \t L: 3.686 \t -- 6231.768\n 0  6600 of 34079 \t L: 3.693 \t -- 6327.438\n 0  6700 of 34079 \t L: 3.633 \t -- 6423.090\n 0  6800 of 34079 \t L: 3.641 \t -- 6518.806\n 0  6900 of 34079 \t L: 3.657 \t -- 6614.570\n 0  7000 of 34079 \t L: 3.639 \t -- 6710.698\n 0  7100 of 34079 \t L: 3.640 \t -- 6806.283\n 0  7200 of 34079 \t L: 3.673 \t -- 6902.224\n 0  7300 of 34079 \t L: 3.643 \t -- 6997.909\n 0  7400 of 34079 \t L: 3.608 \t -- 7093.623\n 0  7500 of 34079 \t L: 3.636 \t -- 7189.743\n 0  7600 of 34079 \t L: 3.673 \t -- 7285.352\n 0  7700 of 34079 \t L: 3.662 \t -- 7381.021\n 0  7800 of 34079 \t L: 3.673 \t -- 7476.558\n 0  7900 of 34079 \t L: 3.642 \t -- 7572.335\n 0  8000 of 34079 \t L: 3.645 \t -- 7668.462\n 0  8100 of 34079 \t L: 3.614 \t -- 7764.213\n 0  8200 of 34079 \t L: 3.627 \t -- 7859.855\n 0  8300 of 34079 \t L: 3.622 \t -- 7955.275\n 0  8400 of 34079 \t L: 3.628 \t -- 8051.033\n 0  8500 of 34079 \t L: 3.624 \t -- 8147.205\n 0  8600 of 34079 \t L: 3.648 \t -- 8242.878\n 0  8700 of 34079 \t L: 3.652 \t -- 8338.559\n 0  8800 of 34079 \t L: 3.599 \t -- 8434.171\n 0  8900 of 34079 \t L: 3.614 \t -- 8529.793\n 0  9000 of 34079 \t L: 3.639 \t -- 8625.898\n 0  9100 of 34079 \t L: 3.600 \t -- 8721.526\n 0  9200 of 34079 \t L: 3.634 \t -- 8817.203\n 0  9300 of 34079 \t L: 3.594 \t -- 8912.948\n 0  9400 of 34079 \t L: 3.595 \t -- 9008.623\n 0  9500 of 34079 \t L: 3.627 \t -- 9104.630\n 0  9600 of 34079 \t L: 3.604 \t -- 9200.353\n 0  9700 of 34079 \t L: 3.604 \t -- 9296.072\n 0  9800 of 34079 \t L: 3.610 \t -- 9391.695\n 0  9900 of 34079 \t L: 3.579 \t -- 9487.413\n 0 10000 of 34079 \t L: 3.589 \t -- 9583.373\n 0 10100 of 34079 \t L: 3.615 \t -- 9679.542\n 0 10200 of 34079 \t L: 3.597 \t -- 9775.229\n 0 10300 of 34079 \t L: 3.628 \t -- 9870.963\n 0 10400 of 34079 \t L: 3.617 \t -- 9966.547\n 0 10500 of 34079 \t L: 3.590 \t -- 10062.149\n 0 10600 of 34079 \t L: 3.602 \t -- 10158.430\n 0 10700 of 34079 \t L: 3.595 \t -- 10254.210\n 0 10800 of 34079 \t L: 3.594 \t -- 10350.079\n 0 10900 of 34079 \t L: 3.608 \t -- 10445.927\n 0 11000 of 34079 \t L: 3.605 \t -- 10541.563\n 0 11100 of 34079 \t L: 3.594 \t -- 10637.588\n 0 11200 of 34079 \t L: 3.582 \t -- 10733.296\n 0 11300 of 34079 \t L: 3.578 \t -- 10828.963\n 0 11400 of 34079 \t L: 3.606 \t -- 10924.932\n 0 11500 of 34079 \t L: 3.582 \t -- 11020.609\n 0 11600 of 34079 \t L: 3.535 \t -- 11117.004\n 0 11700 of 34079 \t L: 3.566 \t -- 11212.761\n 0 11800 of 34079 \t L: 3.587 \t -- 11308.448\n 0 11900 of 34079 \t L: 3.588 \t -- 11404.134\n 0 12000 of 34079 \t L: 3.550 \t -- 11499.915\n 0 12100 of 34079 \t L: 3.578 \t -- 11596.166\n 0 12200 of 34079 \t L: 3.570 \t -- 11691.894\n 0 12300 of 34079 \t L: 3.567 \t -- 11787.614\n 0 12400 of 34079 \t L: 3.604 \t -- 11883.392\n 0 12500 of 34079 \t L: 3.559 \t -- 11979.226\n 0 12600 of 34079 \t L: 3.548 \t -- 12075.414\n 0 12700 of 34079 \t L: 3.549 \t -- 12171.105\n 0 12800 of 34079 \t L: 3.536 \t -- 12266.783\n 0 12900 of 34079 \t L: 3.555 \t -- 12362.635\n 0 13000 of 34079 \t L: 3.552 \t -- 12458.107\n 0 13100 of 34079 \t L: 3.571 \t -- 12553.692\n 0 13200 of 34079 \t L: 3.558 \t -- 12649.847\n 0 13300 of 34079 \t L: 3.567 \t -- 12745.515\n 0 13400 of 34079 \t L: 3.574 \t -- 12841.153\n 0 13500 of 34079 \t L: 3.550 \t -- 12937.038\n 0 13600 of 34079 \t L: 3.545 \t -- 13032.633\n 0 13700 of 34079 \t L: 3.564 \t -- 13128.770\n 0 13800 of 34079 \t L: 3.575 \t -- 13224.486\n 0 13900 of 34079 \t L: 3.536 \t -- 13320.047\n 0 14000 of 34079 \t L: 3.569 \t -- 13415.801\n 0 14100 of 34079 \t L: 3.546 \t -- 13511.669\n 0 14200 of 34079 \t L: 3.574 \t -- 13607.880\n 0 14300 of 34079 \t L: 3.521 \t -- 13703.607\n 0 14400 of 34079 \t L: 3.564 \t -- 13799.259\n 0 14500 of 34079 \t L: 3.562 \t -- 13894.988\n 0 14600 of 34079 \t L: 3.574 \t -- 13990.687\n 0 14700 of 34079 \t L: 3.529 \t -- 14086.796\n 0 14800 of 34079 \t L: 3.526 \t -- 14182.500\n 0 14900 of 34079 \t L: 3.537 \t -- 14278.401\n 0 15000 of 34079 \t L: 3.518 \t -- 14374.160\n 0 15100 of 34079 \t L: 3.536 \t -- 14469.826\n 0 15200 of 34079 \t L: 3.519 \t -- 14566.323\n 0 15300 of 34079 \t L: 3.552 \t -- 14662.128\n 0 15400 of 34079 \t L: 3.511 \t -- 14758.056\n 0 15500 of 34079 \t L: 3.578 \t -- 14853.817\n 0 15600 of 34079 \t L: 3.545 \t -- 14949.645\n 0 15700 of 34079 \t L: 3.551 \t -- 15045.995\n 0 15800 of 34079 \t L: 3.554 \t -- 15141.695\n 0 15900 of 34079 \t L: 3.536 \t -- 15237.317\n 0 16000 of 34079 \t L: 3.497 \t -- 15333.189\n 0 16100 of 34079 \t L: 3.505 \t -- 15428.949\n 0 16200 of 34079 \t L: 3.555 \t -- 15525.224\n 0 16300 of 34079 \t L: 3.501 \t -- 15620.965\n 0 16400 of 34079 \t L: 3.530 \t -- 15716.758\n 0 16500 of 34079 \t L: 3.509 \t -- 15812.694\n 0 16600 of 34079 \t L: 3.535 \t -- 15908.482\n 0 16700 of 34079 \t L: 3.515 \t -- 16004.246\n 0 16800 of 34079 \t L: 3.549 \t -- 16100.573\n 0 16900 of 34079 \t L: 3.530 \t -- 16196.322\n 0 17000 of 34079 \t L: 3.525 \t -- 16292.163\n 0 17100 of 34079 \t L: 3.521 \t -- 16388.001\n 0 17200 of 34079 \t L: 3.523 \t -- 16483.808\n 0 17300 of 34079 \t L: 3.513 \t -- 16580.068\n 0 17400 of 34079 \t L: 3.550 \t -- 16675.883\n 0 17500 of 34079 \t L: 3.490 \t -- 16771.632\n 0 17600 of 34079 \t L: 3.499 \t -- 16867.443\n 0 17700 of 34079 \t L: 3.544 \t -- 16963.205\n 0 17800 of 34079 \t L: 3.518 \t -- 17059.402\n 0 17900 of 34079 \t L: 3.501 \t -- 17155.231\n 0 18000 of 34079 \t L: 3.511 \t -- 17250.979\n 0 18100 of 34079 \t L: 3.495 \t -- 17346.856\n 0 18200 of 34079 \t L: 3.555 \t -- 17442.752\n 0 18300 of 34079 \t L: 3.507 \t -- 17539.025\n 0 18400 of 34079 \t L: 3.528 \t -- 17634.773\n 0 18500 of 34079 \t L: 3.495 \t -- 17730.556\n 0 18600 of 34079 \t L: 3.544 \t -- 17826.304\n 0 18700 of 34079 \t L: 3.520 \t -- 17922.052\n 0 18800 of 34079 \t L: 3.511 \t -- 18018.260\n 0 18900 of 34079 \t L: 3.503 \t -- 18114.002\n 0 19000 of 34079 \t L: 3.482 \t -- 18209.620\n 0 19100 of 34079 \t L: 3.526 \t -- 18305.451\n 0 19200 of 34079 \t L: 3.500 \t -- 18401.450\n 0 19300 of 34079 \t L: 3.516 \t -- 18497.593\n 0 19400 of 34079 \t L: 3.506 \t -- 18593.460\n 0 19500 of 34079 \t L: 3.488 \t -- 18688.915\n 0 19600 of 34079 \t L: 3.499 \t -- 18784.502\n 0 19700 of 34079 \t L: 3.481 \t -- 18880.196\n 0 19800 of 34079 \t L: 3.545 \t -- 18976.245\n 0 19900 of 34079 \t L: 3.501 \t -- 19071.831\n 0 20000 of 34079 \t L: 3.507 \t -- 19167.543\n 0 20100 of 34079 \t L: 3.527 \t -- 19263.265\n 0 20200 of 34079 \t L: 3.478 \t -- 19359.221\n 0 20300 of 34079 \t L: 3.521 \t -- 19454.955\n 0 20400 of 34079 \t L: 3.486 \t -- 19551.093\n 0 20500 of 34079 \t L: 3.484 \t -- 19646.623\n 0 20600 of 34079 \t L: 3.499 \t -- 19742.161\n 0 20700 of 34079 \t L: 3.512 \t -- 19837.927\n 0 20800 of 34079 \t L: 3.513 \t -- 19933.598\n 0 20900 of 34079 \t L: 3.491 \t -- 20029.674\n 0 21000 of 34079 \t L: 3.513 \t -- 20125.427\n 0 21100 of 34079 \t L: 3.490 \t -- 20221.134\n 0 21200 of 34079 \t L: 3.506 \t -- 20316.808\n 0 21300 of 34079 \t L: 3.470 \t -- 20412.512\n 0 21400 of 34079 \t L: 3.493 \t -- 20508.649\n 0 21500 of 34079 \t L: 3.473 \t -- 20604.427\n 0 21600 of 34079 \t L: 3.491 \t -- 20700.135\n 0 21700 of 34079 \t L: 3.518 \t -- 20795.796\n 0 21800 of 34079 \t L: 3.504 \t -- 20891.624\n 0 21900 of 34079 \t L: 3.496 \t -- 20987.887\n 0 22000 of 34079 \t L: 3.475 \t -- 21083.572\n 0 22100 of 34079 \t L: 3.472 \t -- 21179.456\n 0 22200 of 34079 \t L: 3.503 \t -- 21275.248\n 0 22300 of 34079 \t L: 3.470 \t -- 21371.021\n 0 22400 of 34079 \t L: 3.472 \t -- 21467.472\n 0 22500 of 34079 \t L: 3.472 \t -- 21563.190\n 0 22600 of 34079 \t L: 3.475 \t -- 21658.840\n 0 22700 of 34079 \t L: 3.507 \t -- 21754.540\n 0 22800 of 34079 \t L: 3.491 \t -- 21850.398\n 0 22900 of 34079 \t L: 3.491 \t -- 21946.571\n 0 23000 of 34079 \t L: 3.488 \t -- 22042.371\n 0 23100 of 34079 \t L: 3.495 \t -- 22137.998\n 0 23200 of 34079 \t L: 3.501 \t -- 22233.681\n 0 23300 of 34079 \t L: 3.465 \t -- 22329.292\n 0 23400 of 34079 \t L: 3.482 \t -- 22424.904\n 0 23500 of 34079 \t L: 3.479 \t -- 22521.103\n 0 23600 of 34079 \t L: 3.495 \t -- 22616.834\n 0 23700 of 34079 \t L: 3.493 \t -- 22712.517\n 0 23800 of 34079 \t L: 3.487 \t -- 22808.244\n 0 23900 of 34079 \t L: 3.488 \t -- 22903.885\n 0 24000 of 34079 \t L: 3.499 \t -- 23000.237\n 0 24100 of 34079 \t L: 3.473 \t -- 23096.022\n 0 24200 of 34079 \t L: 3.493 \t -- 23191.786\n 0 24300 of 34079 \t L: 3.486 \t -- 23287.549\n 0 24400 of 34079 \t L: 3.517 \t -- 23383.282\n 0 24500 of 34079 \t L: 3.516 \t -- 23479.344\n 0 24600 of 34079 \t L: 3.484 \t -- 23575.084\n 0 24700 of 34079 \t L: 3.489 \t -- 23670.811\n 0 24800 of 34079 \t L: 3.475 \t -- 23766.610\n 0 24900 of 34079 \t L: 3.474 \t -- 23862.401\n 0 25000 of 34079 \t L: 3.479 \t -- 23958.826\n 0 25100 of 34079 \t L: 3.485 \t -- 24054.427\n 0 25200 of 34079 \t L: 3.478 \t -- 24150.202\n 0 25300 of 34079 \t L: 3.473 \t -- 24245.926\n 0 25400 of 34079 \t L: 3.504 \t -- 24341.624\n 0 25500 of 34079 \t L: 3.436 \t -- 24437.838\n 0 25600 of 34079 \t L: 3.451 \t -- 24533.678\n 0 25700 of 34079 \t L: 3.464 \t -- 24629.430\n 0 25800 of 34079 \t L: 3.502 \t -- 24724.977\n 0 25900 of 34079 \t L: 3.493 \t -- 24820.783\n 0 26000 of 34079 \t L: 3.509 \t -- 24916.996\n 0 26100 of 34079 \t L: 3.474 \t -- 25012.596\n 0 26200 of 34079 \t L: 3.474 \t -- 25108.337\n 0 26300 of 34079 \t L: 3.481 \t -- 25204.152\n 0 26400 of 34079 \t L: 3.486 \t -- 25299.838\n 0 26500 of 34079 \t L: 3.473 \t -- 25395.974\n 0 26600 of 34079 \t L: 3.449 \t -- 25491.452\n 0 26700 of 34079 \t L: 3.443 \t -- 25587.261\n 0 26800 of 34079 \t L: 3.501 \t -- 25683.008\n 0 26900 of 34079 \t L: 3.452 \t -- 25778.588\n 0 27000 of 34079 \t L: 3.453 \t -- 25874.388\n 0 27100 of 34079 \t L: 3.463 \t -- 25970.518\n 0 27200 of 34079 \t L: 3.453 \t -- 26066.404\n 0 27300 of 34079 \t L: 3.487 \t -- 26162.166\n 0 27400 of 34079 \t L: 3.489 \t -- 26257.840\n 0 27500 of 34079 \t L: 3.484 \t -- 26353.522\n 0 27600 of 34079 \t L: 3.434 \t -- 26449.566\n 0 27700 of 34079 \t L: 3.483 \t -- 26545.212\n 0 27800 of 34079 \t L: 3.466 \t -- 26640.895\n 0 27900 of 34079 \t L: 3.440 \t -- 26736.449\n 0 28000 of 34079 \t L: 3.451 \t -- 26832.207\n 0 28100 of 34079 \t L: 3.453 \t -- 26928.317\n 0 28200 of 34079 \t L: 3.492 \t -- 27024.194\n 0 28300 of 34079 \t L: 3.496 \t -- 27120.027\n 0 28400 of 34079 \t L: 3.448 \t -- 27215.810\n 0 28500 of 34079 \t L: 3.440 \t -- 27311.545\n 0 28600 of 34079 \t L: 3.456 \t -- 27407.752\n 0 28700 of 34079 \t L: 3.448 \t -- 27503.490\n 0 28800 of 34079 \t L: 3.473 \t -- 27599.328\n 0 28900 of 34079 \t L: 3.368 \t -- 27695.412\n 0 29000 of 34079 \t L: 3.318 \t -- 27791.713\n 0 29100 of 34079 \t L: 3.291 \t -- 27888.635\n 0 29200 of 34079 \t L: 3.242 \t -- 27984.635\n 0 29300 of 34079 \t L: 3.284 \t -- 28080.832\n 0 29400 of 34079 \t L: 3.298 \t -- 28176.826\n 0 29500 of 34079 \t L: 3.299 \t -- 28272.771\n 0 29600 of 34079 \t L: 3.254 \t -- 28369.189\n 0 29700 of 34079 \t L: 3.335 \t -- 28465.215\n 0 29800 of 34079 \t L: 3.304 \t -- 28561.020\n 0 29900 of 34079 \t L: 3.315 \t -- 28656.936\n 0 30000 of 34079 \t L: 3.315 \t -- 28752.774\n 0 30100 of 34079 \t L: 3.320 \t -- 28849.021\n 0 30200 of 34079 \t L: 3.307 \t -- 28945.016\n 0 30300 of 34079 \t L: 3.238 \t -- 29040.787\n 0 30400 of 34079 \t L: 3.484 \t -- 29136.429\n 0 30500 of 34079 \t L: 3.456 \t -- 29232.057\n 0 30600 of 34079 \t L: 3.454 \t -- 29327.590\n 0 30700 of 34079 \t L: 3.419 \t -- 29423.883\n 0 30800 of 34079 \t L: 3.468 \t -- 29519.477\n 0 30900 of 34079 \t L: 3.450 \t -- 29615.159\n 0 31000 of 34079 \t L: 3.409 \t -- 29710.816\n 0 31100 of 34079 \t L: 3.444 \t -- 29806.378\n 0 31200 of 34079 \t L: 3.424 \t -- 29902.711\n 0 31300 of 34079 \t L: 3.424 \t -- 29998.270\n 0 31400 of 34079 \t L: 3.463 \t -- 30093.922\n 0 31500 of 34079 \t L: 3.473 \t -- 30189.516\n 0 31600 of 34079 \t L: 3.408 \t -- 30285.241\n 0 31700 of 34079 \t L: 3.413 \t -- 30381.273\n 0 31800 of 34079 \t L: 3.409 \t -- 30476.992\n 0 31900 of 34079 \t L: 3.405 \t -- 30572.509\n 0 32000 of 34079 \t L: 3.430 \t -- 30668.175\n 0 32100 of 34079 \t L: 3.374 \t -- 30763.700\n 0 32200 of 34079 \t L: 3.412 \t -- 30859.812\n 0 32300 of 34079 \t L: 3.417 \t -- 30955.503\n 0 32400 of 34079 \t L: 3.431 \t -- 31051.108\n 0 32500 of 34079 \t L: 3.414 \t -- 31146.753\n 0 32600 of 34079 \t L: 3.386 \t -- 31242.565\n 0 32700 of 34079 \t L: 3.421 \t -- 31338.730\n 0 32800 of 34079 \t L: 3.403 \t -- 31434.558\n 0 32900 of 34079 \t L: 3.388 \t -- 31530.330\n 0 33000 of 34079 \t L: 3.398 \t -- 31626.056\n 0 33100 of 34079 \t L: 3.446 \t -- 31721.732\n 0 33200 of 34079 \t L: 3.422 \t -- 31817.874\n 0 33300 of 34079 \t L: 3.421 \t -- 31913.453\n 0 33400 of 34079 \t L: 3.433 \t -- 32009.136\n 0 33500 of 34079 \t L: 3.389 \t -- 32104.789\n 0 33600 of 34079 \t L: 3.409 \t -- 32200.690\n 0 33700 of 34079 \t L: 3.432 \t -- 32296.303\n 0 33800 of 34079 \t L: 3.419 \t -- 32392.366\n 0 33900 of 34079 \t L: 3.405 \t -- 32488.030\n 0 34000 of 34079 \t L: 3.424 \t -- 32583.727\nSaving model bart_eli5_task_1\n    0 of   188 \t L: 3.563 \t -- 0.390\n  100 of   188 \t L: 3.458 \t -- 40.151\nTotal \t L: 3.462 \t -- 75.004\n","output_type":"stream"}]},{"cell_type":"code","source":"predicted = []\nreference = []\n\n# Generate answers for the full test set\nfor i in range(len(eli5_valid)):\n    # create support document with the dense index\n    question = eli5_valid[i]['question']\n    support_doc = \"<P> \" + \" <P> \".join([str(p) for p in eli5_valid[i][\"ctxs\"]])\n    # concatenate question and support document into BART input\n    question_doc = \"question: {} context: {}\".format(question, support_doc)\n    # generate an answer with beam search\n    answer = qa_s2s_generate(question_doc, qa_s2s_model, qa_s2s_tokenizer,\n                             num_answers=1,num_beams=8,min_len=96,\n                             max_len=256,max_input_length=1024,device=\"cuda:0\")\n    predicted += [answer[0]]\n    reference += [eli5_valid[i]['answers'][0]]\n    if i % 100 == 0: print(\"Step: \",i,\"/\",len(eli5_valid))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-02-01T07:37:40.665547Z","iopub.execute_input":"2023-02-01T07:37:40.666696Z","iopub.status.idle":"2023-02-01T08:16:45.227104Z","shell.execute_reply.started":"2023-02-01T07:37:40.666650Z","shell.execute_reply":"2023-02-01T08:16:45.226127Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Step:  0 / 1507\nStep:  100 / 1507\nStep:  200 / 1507\nStep:  300 / 1507\nStep:  400 / 1507\nStep:  500 / 1507\nStep:  600 / 1507\nStep:  700 / 1507\nStep:  800 / 1507\nStep:  900 / 1507\nStep:  1000 / 1507\nStep:  1100 / 1507\nStep:  1200 / 1507\nStep:  1300 / 1507\nStep:  1400 / 1507\nStep:  1500 / 1507\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip -q install rouge","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-02-01T08:16:45.228474Z","iopub.execute_input":"2023-02-01T08:16:45.228924Z","iopub.status.idle":"2023-02-01T08:16:58.620353Z","shell.execute_reply.started":"2023-02-01T08:16:45.228887Z","shell.execute_reply":"2023-02-01T08:16:58.619136Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from nltk import PorterStemmer\nfrom rouge import Rouge\nfrom spacy.lang.en import English\nfrom time import time\n\nstemmer = PorterStemmer()\nrouge = Rouge()\nnlpp = English()\ntokenizer = nlpp.tokenizer\n\ndef compute_rouge_eli5(compare_list):\n    preds = [\" \".join([stemmer.stem(str(w))for w in tokenizer(pred)])for gold, pred in compare_list]\n    golds = [\" \".join([stemmer.stem(str(w))for w in tokenizer(gold)])for gold, pred in compare_list]\n    scores = rouge.get_scores(hyps=preds, refs=golds, avg=True)\n    return scores\n\n\ncompare_list = [(g, p) for p, g in zip(predicted, reference)]\nscores = compute_rouge_eli5(compare_list)\ndf = pd.DataFrame({\n    'rouge1': [scores['rouge-1']['p'], scores['rouge-1']['r'], scores['rouge-1']['f']],\n    'rouge2': [scores['rouge-2']['p'], scores['rouge-2']['r'], scores['rouge-2']['f']],\n    'rougeL': [scores['rouge-l']['p'], scores['rouge-l']['r'], scores['rouge-l']['f']],\n}, index=[ 'P', 'R', 'F'])\ndf.style.format({'rouge1': \"{:.4f}\", 'rouge2': \"{:.4f}\", 'rougeL': \"{:.4f}\"})","metadata":{"_uuid":"152e6df1-7b2e-4b54-87ab-65dc7f673786","_cell_guid":"cf566b13-0185-457f-b773-d0c0d076be7d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-02-01T08:16:58.623902Z","iopub.execute_input":"2023-02-01T08:16:58.624587Z","iopub.status.idle":"2023-02-01T08:17:35.985119Z","shell.execute_reply.started":"2023-02-01T08:16:58.624546Z","shell.execute_reply":"2023-02-01T08:17:35.984169Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<pandas.io.formats.style.Styler at 0x7f58724e9450>","text/html":"<style type=\"text/css\">\n</style>\n<table id=\"T_1a347_\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th class=\"col_heading level0 col0\" >rouge1</th>\n      <th class=\"col_heading level0 col1\" >rouge2</th>\n      <th class=\"col_heading level0 col2\" >rougeL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_1a347_level0_row0\" class=\"row_heading level0 row0\" >P</th>\n      <td id=\"T_1a347_row0_col0\" class=\"data row0 col0\" >0.3800</td>\n      <td id=\"T_1a347_row0_col1\" class=\"data row0 col1\" >0.0723</td>\n      <td id=\"T_1a347_row0_col2\" class=\"data row0 col2\" >0.3404</td>\n    </tr>\n    <tr>\n      <th id=\"T_1a347_level0_row1\" class=\"row_heading level0 row1\" >R</th>\n      <td id=\"T_1a347_row1_col0\" class=\"data row1 col0\" >0.2566</td>\n      <td id=\"T_1a347_row1_col1\" class=\"data row1 col1\" >0.0523</td>\n      <td id=\"T_1a347_row1_col2\" class=\"data row1 col2\" >0.2278</td>\n    </tr>\n    <tr>\n      <th id=\"T_1a347_level0_row2\" class=\"row_heading level0 row2\" >F</th>\n      <td id=\"T_1a347_row2_col0\" class=\"data row2 col0\" >0.2757</td>\n      <td id=\"T_1a347_row2_col1\" class=\"data row2 col1\" >0.0514</td>\n      <td id=\"T_1a347_row2_col2\" class=\"data row2 col2\" >0.2454</td>\n    </tr>\n  </tbody>\n</table>\n"},"metadata":{}}]}]}